{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TestO28_4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "K70hAckqg0EA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wVIx_KIigxPV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.regularizers import l2\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.initializers import VarianceScaling\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HqrJuk5643Zf",
        "colab_type": "code",
        "outputId": "739f6f5c-db09-42a6-a9ca-b8bfe4f752ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3WPiElyASpwX",
        "colab_type": "code",
        "outputId": "9b9b6afa-9ba6-4279-aff8-c85127f98f1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "cd drive/'My Drive'/'Colab Notebooks'/test"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'drive/My Drive/Colab Notebooks/test'\n",
            "/content/drive/My Drive/Colab Notebooks/test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UNHw6luQg3gc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
        "# backend\n",
        "import tensorflow as tf\n",
        "from keras import backend as k\n",
        "\n",
        "# Don't pre-allocate memory; allocate as-needed\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "# Create a session with the above options specified.\n",
        "k.tensorflow_backend.set_session(tf.Session(config=config))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dsO_yGxcg5D8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "epochs = 50\n",
        "l = 40\n",
        "num_filter = 24\n",
        "compression = 0.5\n",
        "dropout_rate = 0.2\n",
        "weight_decay = 1E-4 # It is used in l2 regularisation, this specific value is most commonly used"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mB7o3zu1g6eT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load CIFAR10 Data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-3uiqsutzBN3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=True,\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9spEFgQJujtx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "` VarianceScaling(scale = 2.0, mode = 'fan_in', distribution = 'normal', seed = None)` is used for MSRA initialization as per the dense net paper.\n",
        "Dropout is used to solve overfitting along with it l2 regulirisation is used to increase `val_acc` "
      ]
    },
    {
      "metadata": {
        "id": "ee-sge5Kg7vr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Dense Block\n",
        "def add_denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    temp = input\n",
        "    for _ in range(l):\n",
        "        BatchNorm = BatchNormalization(axis = -1, gamma_regularizer=l2(weight_decay), beta_regularizer=l2(weight_decay))(temp)\n",
        "        relu = Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = Conv2D( num_filter, (3,3), kernel_initializer = VarianceScaling(scale = 2.0, mode = 'fan_in', distribution = 'normal', seed = None), use_bias=False ,padding='same' )(relu) # msra initialization\n",
        "        if dropout_rate>0:\n",
        "          Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
        "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OOP6IPsGhBwb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def add_transition(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization(axis = -1, gamma_regularizer=l2(weight_decay), beta_regularizer=l2(weight_decay))(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), kernel_initializer = VarianceScaling(scale = 2.0, mode = 'fan_in', distribution = 'normal', seed = None), use_bias=False ,padding='same')(relu)\n",
        "    if dropout_rate>0:\n",
        "      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    \n",
        "    return avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0RaKFpubhDIC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization(axis = -1, gamma_regularizer=l2(weight_decay), beta_regularizer=l2(weight_decay))(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    flat = Flatten()(AvgPooling)\n",
        "    output = Dense(num_classes, activation='softmax')(flat)\n",
        "    \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "anPCpQWhhGb7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_filter = 16 # To get as many major features as possible the number of filter  is more (16)\n",
        "dropout_rate = 0.2\n",
        "l = 18\n",
        "input = Input(shape=(img_height, img_width, channel,))\n",
        "First_Conv2D = Conv2D(num_filter, (3,3), kernel_initializer = VarianceScaling(scale = 2.0, mode = 'fan_in', distribution = 'normal', seed = None), use_bias=False ,padding='same')(input)\n",
        "\n",
        "num_filter = 12\n",
        "\n",
        "First_Block = add_denseblock(First_Conv2D, num_filter, dropout_rate)\n",
        "First_Transition = add_transition(First_Block, num_filter, dropout_rate)\n",
        "\n",
        "Second_Block = add_denseblock(First_Transition, num_filter, dropout_rate)\n",
        "Second_Transition = add_transition(Second_Block, num_filter, dropout_rate)\n",
        "\n",
        "Third_Block = add_denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "Third_Transition = add_transition(Third_Block, num_filter, dropout_rate)\n",
        "\n",
        "Last_Block = add_denseblock(Third_Transition,  num_filter, dropout_rate)\n",
        "output = output_layer(Last_Block)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1kFh7pdxhNtT",
        "colab_type": "code",
        "outputId": "838babfb-1e5d-4a43-afd0-e56292ff5f18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 15500
        }
      },
      "cell_type": "code",
      "source": [
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 16)   432         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 12)   1728        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 32, 32, 12)   0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 28)   0           conv2d_1[0][0]                   \n",
            "                                                                 dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 28)   112         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 28)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 12)   3024        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 32, 32, 12)   0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 40)   0           concatenate_1[0][0]              \n",
            "                                                                 dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 40)   160         concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 40)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 12)   4320        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 32, 32, 12)   0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 32, 32, 52)   0           concatenate_2[0][0]              \n",
            "                                                                 dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 52)   208         concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 52)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 12)   5616        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 32, 32, 12)   0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 32, 32, 64)   0           concatenate_3[0][0]              \n",
            "                                                                 dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 64)   256         concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 64)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 12)   6912        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 32, 32, 12)   0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 32, 32, 76)   0           concatenate_4[0][0]              \n",
            "                                                                 dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 76)   304         concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 76)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 12)   8208        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 32, 32, 12)   0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 32, 32, 88)   0           concatenate_5[0][0]              \n",
            "                                                                 dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 88)   352         concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 88)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 12)   9504        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 32, 32, 12)   0           conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 32, 32, 100)  0           concatenate_6[0][0]              \n",
            "                                                                 dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 100)  400         concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 100)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 12)   10800       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 32, 32, 12)   0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 32, 32, 112)  0           concatenate_7[0][0]              \n",
            "                                                                 dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 112)  448         concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 112)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 12)   12096       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 32, 32, 12)   0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 32, 32, 124)  0           concatenate_8[0][0]              \n",
            "                                                                 dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 124)  496         concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 124)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 12)   13392       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 32, 32, 12)   0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 32, 32, 136)  0           concatenate_9[0][0]              \n",
            "                                                                 dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 32, 32, 136)  544         concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32, 32, 136)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 12)   14688       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 32, 32, 12)   0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 32, 32, 148)  0           concatenate_10[0][0]             \n",
            "                                                                 dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 32, 32, 148)  592         concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 32, 32, 148)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 12)   15984       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 32, 32, 12)   0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 32, 32, 160)  0           concatenate_11[0][0]             \n",
            "                                                                 dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 32, 32, 160)  640         concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 32, 32, 160)  0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 32, 32, 12)   17280       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 32, 32, 12)   0           conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 32, 32, 172)  0           concatenate_12[0][0]             \n",
            "                                                                 dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 32, 32, 172)  688         concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 32, 32, 172)  0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 32, 32, 12)   18576       activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 32, 32, 12)   0           conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 32, 32, 184)  0           concatenate_13[0][0]             \n",
            "                                                                 dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 32, 32, 184)  736         concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 32, 32, 184)  0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 32, 32, 12)   19872       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 32, 32, 12)   0           conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 32, 32, 196)  0           concatenate_14[0][0]             \n",
            "                                                                 dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 32, 32, 196)  784         concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 32, 32, 196)  0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 32, 32, 12)   21168       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 32, 32, 12)   0           conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 32, 32, 208)  0           concatenate_15[0][0]             \n",
            "                                                                 dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 32, 32, 208)  832         concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 32, 32, 208)  0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 32, 32, 12)   22464       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 32, 32, 12)   0           conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 32, 32, 220)  0           concatenate_16[0][0]             \n",
            "                                                                 dropout_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 32, 32, 220)  880         concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 32, 32, 220)  0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 32, 32, 12)   23760       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 32, 32, 12)   0           conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 32, 32, 232)  0           concatenate_17[0][0]             \n",
            "                                                                 dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 32, 32, 232)  928         concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 32, 32, 232)  0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 32, 32, 6)    1392        activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 32, 32, 6)    0           conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 16, 16, 6)    0           dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 16, 16, 6)    24          average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 16, 16, 6)    0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 16, 16, 12)   648         activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, 16, 16, 12)   0           conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 16, 16, 18)   0           average_pooling2d_1[0][0]        \n",
            "                                                                 dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 16, 16, 18)   72          concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 16, 16, 18)   0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 16, 16, 12)   1944        activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 16, 16, 12)   0           conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (None, 16, 16, 30)   0           concatenate_19[0][0]             \n",
            "                                                                 dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 16, 16, 30)   120         concatenate_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 16, 16, 30)   0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 16, 16, 12)   3240        activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, 16, 16, 12)   0           conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_21 (Concatenate)    (None, 16, 16, 42)   0           concatenate_20[0][0]             \n",
            "                                                                 dropout_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 16, 16, 42)   168         concatenate_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 16, 16, 42)   0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 16, 16, 12)   4536        activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_23 (Dropout)            (None, 16, 16, 12)   0           conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (None, 16, 16, 54)   0           concatenate_21[0][0]             \n",
            "                                                                 dropout_23[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 16, 16, 54)   216         concatenate_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 16, 16, 54)   0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 16, 16, 12)   5832        activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 16, 16, 12)   0           conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_23 (Concatenate)    (None, 16, 16, 66)   0           concatenate_22[0][0]             \n",
            "                                                                 dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 16, 16, 66)   264         concatenate_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 16, 16, 66)   0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 16, 16, 12)   7128        activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 16, 16, 12)   0           conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_24 (Concatenate)    (None, 16, 16, 78)   0           concatenate_23[0][0]             \n",
            "                                                                 dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 16, 16, 78)   312         concatenate_24[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 16, 16, 78)   0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 16, 16, 12)   8424        activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 16, 16, 12)   0           conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_25 (Concatenate)    (None, 16, 16, 90)   0           concatenate_24[0][0]             \n",
            "                                                                 dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 16, 16, 90)   360         concatenate_25[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 16, 16, 90)   0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 16, 16, 12)   9720        activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_27 (Dropout)            (None, 16, 16, 12)   0           conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_26 (Concatenate)    (None, 16, 16, 102)  0           concatenate_25[0][0]             \n",
            "                                                                 dropout_27[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 16, 16, 102)  408         concatenate_26[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 16, 16, 102)  0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 16, 16, 12)   11016       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_28 (Dropout)            (None, 16, 16, 12)   0           conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_27 (Concatenate)    (None, 16, 16, 114)  0           concatenate_26[0][0]             \n",
            "                                                                 dropout_28[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 16, 16, 114)  456         concatenate_27[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 16, 16, 114)  0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 16, 16, 12)   12312       activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_29 (Dropout)            (None, 16, 16, 12)   0           conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_28 (Concatenate)    (None, 16, 16, 126)  0           concatenate_27[0][0]             \n",
            "                                                                 dropout_29[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 16, 16, 126)  504         concatenate_28[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 16, 16, 126)  0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 16, 16, 12)   13608       activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_30 (Dropout)            (None, 16, 16, 12)   0           conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_29 (Concatenate)    (None, 16, 16, 138)  0           concatenate_28[0][0]             \n",
            "                                                                 dropout_30[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 16, 16, 138)  552         concatenate_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 16, 16, 138)  0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 16, 16, 12)   14904       activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_31 (Dropout)            (None, 16, 16, 12)   0           conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_30 (Concatenate)    (None, 16, 16, 150)  0           concatenate_29[0][0]             \n",
            "                                                                 dropout_31[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 16, 16, 150)  600         concatenate_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 16, 16, 150)  0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 16, 16, 12)   16200       activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_32 (Dropout)            (None, 16, 16, 12)   0           conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_31 (Concatenate)    (None, 16, 16, 162)  0           concatenate_30[0][0]             \n",
            "                                                                 dropout_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 16, 16, 162)  648         concatenate_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 16, 16, 162)  0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 16, 16, 12)   17496       activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_33 (Dropout)            (None, 16, 16, 12)   0           conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_32 (Concatenate)    (None, 16, 16, 174)  0           concatenate_31[0][0]             \n",
            "                                                                 dropout_33[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 16, 16, 174)  696         concatenate_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 16, 16, 174)  0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 16, 16, 12)   18792       activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_34 (Dropout)            (None, 16, 16, 12)   0           conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_33 (Concatenate)    (None, 16, 16, 186)  0           concatenate_32[0][0]             \n",
            "                                                                 dropout_34[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 16, 16, 186)  744         concatenate_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 16, 16, 186)  0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 16, 16, 12)   20088       activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_35 (Dropout)            (None, 16, 16, 12)   0           conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_34 (Concatenate)    (None, 16, 16, 198)  0           concatenate_33[0][0]             \n",
            "                                                                 dropout_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 16, 16, 198)  792         concatenate_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 16, 16, 198)  0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 16, 16, 12)   21384       activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_36 (Dropout)            (None, 16, 16, 12)   0           conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_35 (Concatenate)    (None, 16, 16, 210)  0           concatenate_34[0][0]             \n",
            "                                                                 dropout_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 16, 16, 210)  840         concatenate_35[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 16, 16, 210)  0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 16, 16, 12)   22680       activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_37 (Dropout)            (None, 16, 16, 12)   0           conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_36 (Concatenate)    (None, 16, 16, 222)  0           concatenate_35[0][0]             \n",
            "                                                                 dropout_37[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 16, 16, 222)  888         concatenate_36[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 16, 16, 222)  0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 16, 16, 6)    1332        activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 16, 16, 6)    0           conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 8, 8, 6)      0           dropout_38[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 8, 8, 6)      24          average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 8, 8, 6)      0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 8, 8, 12)     648         activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_39 (Dropout)            (None, 8, 8, 12)     0           conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_37 (Concatenate)    (None, 8, 8, 18)     0           average_pooling2d_2[0][0]        \n",
            "                                                                 dropout_39[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 8, 8, 18)     72          concatenate_37[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 8, 8, 18)     0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 8, 8, 12)     1944        activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_40 (Dropout)            (None, 8, 8, 12)     0           conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_38 (Concatenate)    (None, 8, 8, 30)     0           concatenate_37[0][0]             \n",
            "                                                                 dropout_40[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 8, 8, 30)     120         concatenate_38[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 8, 8, 30)     0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 8, 8, 12)     3240        activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_41 (Dropout)            (None, 8, 8, 12)     0           conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_39 (Concatenate)    (None, 8, 8, 42)     0           concatenate_38[0][0]             \n",
            "                                                                 dropout_41[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 8, 8, 42)     168         concatenate_39[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 8, 8, 42)     0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 8, 8, 12)     4536        activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_42 (Dropout)            (None, 8, 8, 12)     0           conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_40 (Concatenate)    (None, 8, 8, 54)     0           concatenate_39[0][0]             \n",
            "                                                                 dropout_42[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 8, 8, 54)     216         concatenate_40[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 8, 8, 54)     0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 8, 8, 12)     5832        activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_43 (Dropout)            (None, 8, 8, 12)     0           conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_41 (Concatenate)    (None, 8, 8, 66)     0           concatenate_40[0][0]             \n",
            "                                                                 dropout_43[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 8, 8, 66)     264         concatenate_41[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 8, 8, 66)     0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 8, 8, 12)     7128        activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_44 (Dropout)            (None, 8, 8, 12)     0           conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_42 (Concatenate)    (None, 8, 8, 78)     0           concatenate_41[0][0]             \n",
            "                                                                 dropout_44[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 8, 8, 78)     312         concatenate_42[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 8, 8, 78)     0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 8, 8, 12)     8424        activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_45 (Dropout)            (None, 8, 8, 12)     0           conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_43 (Concatenate)    (None, 8, 8, 90)     0           concatenate_42[0][0]             \n",
            "                                                                 dropout_45[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 8, 8, 90)     360         concatenate_43[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 8, 8, 90)     0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 8, 8, 12)     9720        activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_46 (Dropout)            (None, 8, 8, 12)     0           conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_44 (Concatenate)    (None, 8, 8, 102)    0           concatenate_43[0][0]             \n",
            "                                                                 dropout_46[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 8, 8, 102)    408         concatenate_44[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 8, 8, 102)    0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 8, 8, 12)     11016       activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_47 (Dropout)            (None, 8, 8, 12)     0           conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_45 (Concatenate)    (None, 8, 8, 114)    0           concatenate_44[0][0]             \n",
            "                                                                 dropout_47[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 8, 8, 114)    456         concatenate_45[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 8, 8, 114)    0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 8, 8, 12)     12312       activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_48 (Dropout)            (None, 8, 8, 12)     0           conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_46 (Concatenate)    (None, 8, 8, 126)    0           concatenate_45[0][0]             \n",
            "                                                                 dropout_48[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 8, 8, 126)    504         concatenate_46[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 8, 8, 126)    0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 8, 8, 12)     13608       activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_49 (Dropout)            (None, 8, 8, 12)     0           conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_47 (Concatenate)    (None, 8, 8, 138)    0           concatenate_46[0][0]             \n",
            "                                                                 dropout_49[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 8, 8, 138)    552         concatenate_47[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 8, 8, 138)    0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 8, 8, 12)     14904       activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_50 (Dropout)            (None, 8, 8, 12)     0           conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_48 (Concatenate)    (None, 8, 8, 150)    0           concatenate_47[0][0]             \n",
            "                                                                 dropout_50[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 8, 8, 150)    600         concatenate_48[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 8, 8, 150)    0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 8, 8, 12)     16200       activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_51 (Dropout)            (None, 8, 8, 12)     0           conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_49 (Concatenate)    (None, 8, 8, 162)    0           concatenate_48[0][0]             \n",
            "                                                                 dropout_51[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 8, 8, 162)    648         concatenate_49[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 8, 8, 162)    0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 8, 8, 12)     17496       activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_52 (Dropout)            (None, 8, 8, 12)     0           conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_50 (Concatenate)    (None, 8, 8, 174)    0           concatenate_49[0][0]             \n",
            "                                                                 dropout_52[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 8, 8, 174)    696         concatenate_50[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 8, 8, 174)    0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 8, 8, 12)     18792       activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_53 (Dropout)            (None, 8, 8, 12)     0           conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_51 (Concatenate)    (None, 8, 8, 186)    0           concatenate_50[0][0]             \n",
            "                                                                 dropout_53[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 8, 8, 186)    744         concatenate_51[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 8, 8, 186)    0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 8, 8, 12)     20088       activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_54 (Dropout)            (None, 8, 8, 12)     0           conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_52 (Concatenate)    (None, 8, 8, 198)    0           concatenate_51[0][0]             \n",
            "                                                                 dropout_54[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 8, 8, 198)    792         concatenate_52[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 8, 8, 198)    0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 8, 8, 12)     21384       activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_55 (Dropout)            (None, 8, 8, 12)     0           conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_53 (Concatenate)    (None, 8, 8, 210)    0           concatenate_52[0][0]             \n",
            "                                                                 dropout_55[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 8, 8, 210)    840         concatenate_53[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 8, 8, 210)    0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 8, 8, 12)     22680       activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_56 (Dropout)            (None, 8, 8, 12)     0           conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_54 (Concatenate)    (None, 8, 8, 222)    0           concatenate_53[0][0]             \n",
            "                                                                 dropout_56[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 8, 8, 222)    888         concatenate_54[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 8, 8, 222)    0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 8, 8, 6)      1332        activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_57 (Dropout)            (None, 8, 8, 6)      0           conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 4, 4, 6)      0           dropout_57[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 4, 4, 6)      24          average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 4, 4, 6)      0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 4, 4, 12)     648         activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_58 (Dropout)            (None, 4, 4, 12)     0           conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_55 (Concatenate)    (None, 4, 4, 18)     0           average_pooling2d_3[0][0]        \n",
            "                                                                 dropout_58[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 4, 4, 18)     72          concatenate_55[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 4, 4, 18)     0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 4, 4, 12)     1944        activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_59 (Dropout)            (None, 4, 4, 12)     0           conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_56 (Concatenate)    (None, 4, 4, 30)     0           concatenate_55[0][0]             \n",
            "                                                                 dropout_59[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 4, 4, 30)     120         concatenate_56[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 4, 4, 30)     0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 4, 4, 12)     3240        activation_60[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_60 (Dropout)            (None, 4, 4, 12)     0           conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_57 (Concatenate)    (None, 4, 4, 42)     0           concatenate_56[0][0]             \n",
            "                                                                 dropout_60[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 4, 4, 42)     168         concatenate_57[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 4, 4, 42)     0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 4, 4, 12)     4536        activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_61 (Dropout)            (None, 4, 4, 12)     0           conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_58 (Concatenate)    (None, 4, 4, 54)     0           concatenate_57[0][0]             \n",
            "                                                                 dropout_61[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 4, 4, 54)     216         concatenate_58[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 4, 4, 54)     0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 4, 4, 12)     5832        activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_62 (Dropout)            (None, 4, 4, 12)     0           conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_59 (Concatenate)    (None, 4, 4, 66)     0           concatenate_58[0][0]             \n",
            "                                                                 dropout_62[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 4, 4, 66)     264         concatenate_59[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 4, 4, 66)     0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 4, 4, 12)     7128        activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_63 (Dropout)            (None, 4, 4, 12)     0           conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_60 (Concatenate)    (None, 4, 4, 78)     0           concatenate_59[0][0]             \n",
            "                                                                 dropout_63[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 4, 4, 78)     312         concatenate_60[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 4, 4, 78)     0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 4, 4, 12)     8424        activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_64 (Dropout)            (None, 4, 4, 12)     0           conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_61 (Concatenate)    (None, 4, 4, 90)     0           concatenate_60[0][0]             \n",
            "                                                                 dropout_64[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 4, 4, 90)     360         concatenate_61[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 4, 4, 90)     0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 4, 4, 12)     9720        activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_65 (Dropout)            (None, 4, 4, 12)     0           conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_62 (Concatenate)    (None, 4, 4, 102)    0           concatenate_61[0][0]             \n",
            "                                                                 dropout_65[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 4, 4, 102)    408         concatenate_62[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 4, 4, 102)    0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 4, 4, 12)     11016       activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_66 (Dropout)            (None, 4, 4, 12)     0           conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_63 (Concatenate)    (None, 4, 4, 114)    0           concatenate_62[0][0]             \n",
            "                                                                 dropout_66[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 4, 4, 114)    456         concatenate_63[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 4, 4, 114)    0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 4, 4, 12)     12312       activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_67 (Dropout)            (None, 4, 4, 12)     0           conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_64 (Concatenate)    (None, 4, 4, 126)    0           concatenate_63[0][0]             \n",
            "                                                                 dropout_67[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 4, 4, 126)    504         concatenate_64[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 4, 4, 126)    0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 4, 4, 12)     13608       activation_68[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_68 (Dropout)            (None, 4, 4, 12)     0           conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_65 (Concatenate)    (None, 4, 4, 138)    0           concatenate_64[0][0]             \n",
            "                                                                 dropout_68[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 4, 4, 138)    552         concatenate_65[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 4, 4, 138)    0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 4, 4, 12)     14904       activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_69 (Dropout)            (None, 4, 4, 12)     0           conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_66 (Concatenate)    (None, 4, 4, 150)    0           concatenate_65[0][0]             \n",
            "                                                                 dropout_69[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 4, 4, 150)    600         concatenate_66[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 4, 4, 150)    0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 4, 4, 12)     16200       activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_70 (Dropout)            (None, 4, 4, 12)     0           conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_67 (Concatenate)    (None, 4, 4, 162)    0           concatenate_66[0][0]             \n",
            "                                                                 dropout_70[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 4, 4, 162)    648         concatenate_67[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 4, 4, 162)    0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 4, 4, 12)     17496       activation_71[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_71 (Dropout)            (None, 4, 4, 12)     0           conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_68 (Concatenate)    (None, 4, 4, 174)    0           concatenate_67[0][0]             \n",
            "                                                                 dropout_71[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 4, 4, 174)    696         concatenate_68[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 4, 4, 174)    0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 4, 4, 12)     18792       activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_72 (Dropout)            (None, 4, 4, 12)     0           conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_69 (Concatenate)    (None, 4, 4, 186)    0           concatenate_68[0][0]             \n",
            "                                                                 dropout_72[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 4, 4, 186)    744         concatenate_69[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 4, 4, 186)    0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 4, 4, 12)     20088       activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_73 (Dropout)            (None, 4, 4, 12)     0           conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_70 (Concatenate)    (None, 4, 4, 198)    0           concatenate_69[0][0]             \n",
            "                                                                 dropout_73[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 4, 4, 198)    792         concatenate_70[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 4, 4, 198)    0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 4, 4, 12)     21384       activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_74 (Dropout)            (None, 4, 4, 12)     0           conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_71 (Concatenate)    (None, 4, 4, 210)    0           concatenate_70[0][0]             \n",
            "                                                                 dropout_74[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 4, 4, 210)    840         concatenate_71[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 4, 4, 210)    0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 4, 4, 12)     22680       activation_75[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_75 (Dropout)            (None, 4, 4, 12)     0           conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_72 (Concatenate)    (None, 4, 4, 222)    0           concatenate_71[0][0]             \n",
            "                                                                 dropout_75[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 4, 4, 222)    888         concatenate_72[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 4, 4, 222)    0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 2, 2, 222)    0           activation_76[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 888)          0           average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           8890        flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 908,042\n",
            "Trainable params: 890,334\n",
            "Non-trainable params: 17,708\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bdncwSUpkOlo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_generator = datagen.flow(x_train, y_train, batch_size=64)\n",
        "test_generator = ImageDataGenerator().flow(x_test, y_test, batch_size=64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b4XOsW3ahSkL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.1, decay=1e-4, momentum=0.9, nesterov=True) # initially these values are set based on the Densenet paper\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MtJvKwbf2Wd6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*Initially we start with low batch size as it introduces stochoistic noise which helps to generalize the model and initial lr is high so that the `val_acc` does not sit on the floor.(As per the paper \"[A walk  through SGD](https://arxiv.org/pdf/1802.08770.pdf)\")*"
      ]
    },
    {
      "metadata": {
        "id": "RxTT6-mQ2WP0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "t632bjGWk8KN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filepath= \"weights.{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "crhGk7kEhXAz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train,\n",
        "                    batch_size=64,\n",
        "                    epochs=2,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_vgT1YcDlBOW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(data_generator,\n",
        "                   steps_per_epoch=50000//64,\n",
        "                    epochs=62,\n",
        "                    verbose=1,\n",
        "                    initial_epoch = 2,\n",
        "                    callbacks=callbacks_list,\n",
        "                    validation_data=test_generator,validation_steps=10000//64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TBbvMgBsKcDT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_weights('weights.62-0.80.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BvgCNgAfmiPD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.07, decay=1e-4, momentum=0.9, nesterov=True) \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wTC608qLuj3O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train,\n",
        "                    batch_size=128,\n",
        "                    epochs=65,\n",
        "                    verbose=1,\n",
        "                    initial_epoch=62,\n",
        "                    callbacks=callbacks_list,\n",
        "                    validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AEyMYPlUGICI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.05, decay=1e-4, momentum=0.9, nesterov=True) \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rSt2DOdbGMXn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(data_generator,\n",
        "                   steps_per_epoch=50000//64,\n",
        "                    epochs=80,\n",
        "                    verbose=1,\n",
        "                    initial_epoch = 65,\n",
        "                    callbacks=callbacks_list,\n",
        "                    validation_data=test_generator,validation_steps=10000//64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qv5YC9Pextcb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_weights('weights.80-0.84.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I-FnoeAkiN4C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.04, decay=1e-4, momentum=0.9, nesterov=True) \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d06WueFQiyze",
        "colab_type": "code",
        "outputId": "958fcc05-ee63-448c-f41d-5679ec61b26d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1482
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(data_generator,\n",
        "                   steps_per_epoch=50000//64,\n",
        "                    epochs= 100,\n",
        "                    verbose=1,\n",
        "                    initial_epoch = 80,\n",
        "                    callbacks=callbacks_list,\n",
        "                    validation_data=test_generator,validation_steps=10000//64)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 81/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py:799: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py:807: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 418s 535ms/step - loss: 0.6129 - acc: 0.8092 - val_loss: 0.8572 - val_acc: 0.7664\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.85567\n",
            "Epoch 82/100\n",
            "781/781 [==============================] - 395s 506ms/step - loss: 0.6030 - acc: 0.8117 - val_loss: 0.9659 - val_acc: 0.7430\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.85567\n",
            "Epoch 83/100\n",
            "781/781 [==============================] - 395s 506ms/step - loss: 0.5950 - acc: 0.8148 - val_loss: 0.6791 - val_acc: 0.8147\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.85567\n",
            "Epoch 84/100\n",
            "781/781 [==============================] - 395s 506ms/step - loss: 0.5882 - acc: 0.8202 - val_loss: 0.7431 - val_acc: 0.8024\n",
            "\n",
            "Epoch 00084: val_acc did not improve from 0.85567\n",
            "Epoch 85/100\n",
            "781/781 [==============================] - 395s 506ms/step - loss: 0.5760 - acc: 0.8208 - val_loss: 0.7062 - val_acc: 0.8050\n",
            "\n",
            "Epoch 00085: val_acc did not improve from 0.85567\n",
            "Epoch 86/100\n",
            "781/781 [==============================] - 395s 505ms/step - loss: 0.5650 - acc: 0.8235 - val_loss: 0.7442 - val_acc: 0.7920\n",
            "\n",
            "Epoch 00086: val_acc did not improve from 0.85567\n",
            "Epoch 87/100\n",
            "781/781 [==============================] - 395s 505ms/step - loss: 0.5694 - acc: 0.8242 - val_loss: 0.5552 - val_acc: 0.8445\n",
            "\n",
            "Epoch 00087: val_acc did not improve from 0.85567\n",
            "Epoch 88/100\n",
            "781/781 [==============================] - 394s 505ms/step - loss: 0.5531 - acc: 0.8295 - val_loss: 0.5670 - val_acc: 0.8380\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.85567\n",
            "Epoch 89/100\n",
            "781/781 [==============================] - 394s 505ms/step - loss: 0.5470 - acc: 0.8299 - val_loss: 0.6294 - val_acc: 0.8274\n",
            "\n",
            "Epoch 00089: val_acc did not improve from 0.85567\n",
            "Epoch 90/100\n",
            "781/781 [==============================] - 395s 505ms/step - loss: 0.5405 - acc: 0.8332 - val_loss: 0.6808 - val_acc: 0.8104\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 0.85567\n",
            "Epoch 91/100\n",
            "781/781 [==============================] - 394s 505ms/step - loss: 0.5352 - acc: 0.8345 - val_loss: 0.5372 - val_acc: 0.8575\n",
            "\n",
            "Epoch 00091: val_acc improved from 0.85567 to 0.85747, saving model to weights.91-0.86.hdf5\n",
            "Epoch 92/100\n",
            "781/781 [==============================] - 394s 505ms/step - loss: 0.5286 - acc: 0.8379 - val_loss: 1.0811 - val_acc: 0.7541\n",
            "\n",
            "Epoch 00092: val_acc did not improve from 0.85747\n",
            "Epoch 93/100\n",
            "781/781 [==============================] - 395s 506ms/step - loss: 0.5272 - acc: 0.8362 - val_loss: 0.5622 - val_acc: 0.8398\n",
            "\n",
            "Epoch 00093: val_acc did not improve from 0.85747\n",
            "Epoch 94/100\n",
            "781/781 [==============================] - 395s 506ms/step - loss: 0.5216 - acc: 0.8394 - val_loss: 0.6182 - val_acc: 0.8289\n",
            "\n",
            "Epoch 00094: val_acc did not improve from 0.85747\n",
            "Epoch 95/100\n",
            "781/781 [==============================] - 394s 505ms/step - loss: 0.5187 - acc: 0.8385 - val_loss: 0.5797 - val_acc: 0.8391\n",
            "\n",
            "Epoch 00095: val_acc did not improve from 0.85747\n",
            "Epoch 96/100\n",
            "781/781 [==============================] - 395s 506ms/step - loss: 0.5147 - acc: 0.8402 - val_loss: 0.5618 - val_acc: 0.8416\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 0.85747\n",
            "Epoch 97/100\n",
            "781/781 [==============================] - 395s 506ms/step - loss: 0.5096 - acc: 0.8418 - val_loss: 0.5425 - val_acc: 0.8493\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 0.85747\n",
            "Epoch 98/100\n",
            "781/781 [==============================] - 394s 505ms/step - loss: 0.5088 - acc: 0.8422 - val_loss: 0.6582 - val_acc: 0.8235\n",
            "\n",
            "Epoch 00098: val_acc did not improve from 0.85747\n",
            "Epoch 99/100\n",
            "781/781 [==============================] - 395s 506ms/step - loss: 0.5056 - acc: 0.8420 - val_loss: 0.5376 - val_acc: 0.8501\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 0.85747\n",
            "Epoch 100/100\n",
            "781/781 [==============================] - 394s 505ms/step - loss: 0.5011 - acc: 0.8448 - val_loss: 0.5034 - val_acc: 0.8596\n",
            "\n",
            "Epoch 00100: val_acc improved from 0.85747 to 0.85958, saving model to weights.100-0.86.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5cd8999908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "4pW2mgKh5dCO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.03, decay=1e-4, momentum=0.9, nesterov=True) \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2LxFmKYT5lrw",
        "colab_type": "code",
        "outputId": "6997e172-6639-402c-d1c7-73a2f0774f24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1482
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(data_generator,\n",
        "                   steps_per_epoch=50000//64,\n",
        "                    epochs= 120,\n",
        "                    verbose=1,\n",
        "                    initial_epoch = 100,\n",
        "                    callbacks=callbacks_list,\n",
        "                    validation_data=test_generator,validation_steps=10000//64)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 101/120\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py:799: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py:807: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 421s 539ms/step - loss: 0.5424 - acc: 0.8314 - val_loss: 0.7355 - val_acc: 0.7960\n",
            "\n",
            "Epoch 00101: val_acc did not improve from 0.85958\n",
            "Epoch 102/120\n",
            "781/781 [==============================] - 395s 506ms/step - loss: 0.5411 - acc: 0.8316 - val_loss: 1.0983 - val_acc: 0.7398\n",
            "\n",
            "Epoch 00102: val_acc did not improve from 0.85958\n",
            "Epoch 103/120\n",
            "781/781 [==============================] - 395s 506ms/step - loss: 0.5298 - acc: 0.8365 - val_loss: 0.7274 - val_acc: 0.8073\n",
            "\n",
            "Epoch 00103: val_acc did not improve from 0.85958\n",
            "Epoch 104/120\n",
            "781/781 [==============================] - 402s 514ms/step - loss: 0.5283 - acc: 0.8382 - val_loss: 0.5938 - val_acc: 0.8401\n",
            "\n",
            "Epoch 00104: val_acc did not improve from 0.85958\n",
            "Epoch 105/120\n",
            "781/781 [==============================] - 394s 505ms/step - loss: 0.5214 - acc: 0.8388 - val_loss: 0.5468 - val_acc: 0.8479\n",
            "\n",
            "Epoch 00105: val_acc did not improve from 0.85958\n",
            "Epoch 106/120\n",
            "781/781 [==============================] - 395s 505ms/step - loss: 0.5107 - acc: 0.8398 - val_loss: 0.6134 - val_acc: 0.8313\n",
            "\n",
            "Epoch 00106: val_acc did not improve from 0.85958\n",
            "Epoch 107/120\n",
            "781/781 [==============================] - 396s 506ms/step - loss: 0.5098 - acc: 0.8419 - val_loss: 0.6220 - val_acc: 0.8340\n",
            "\n",
            "Epoch 00107: val_acc did not improve from 0.85958\n",
            "Epoch 108/120\n",
            "781/781 [==============================] - 395s 505ms/step - loss: 0.5026 - acc: 0.8440 - val_loss: 0.7318 - val_acc: 0.8033\n",
            "\n",
            "Epoch 00108: val_acc did not improve from 0.85958\n",
            "Epoch 109/120\n",
            "781/781 [==============================] - 395s 506ms/step - loss: 0.4991 - acc: 0.8469 - val_loss: 0.5128 - val_acc: 0.8523\n",
            "\n",
            "Epoch 00109: val_acc did not improve from 0.85958\n",
            "Epoch 110/120\n",
            "781/781 [==============================] - 398s 510ms/step - loss: 0.4974 - acc: 0.8481 - val_loss: 0.6549 - val_acc: 0.8263\n",
            "\n",
            "Epoch 00110: val_acc did not improve from 0.85958\n",
            "Epoch 111/120\n",
            "781/781 [==============================] - 394s 505ms/step - loss: 0.4899 - acc: 0.8473 - val_loss: 0.9490 - val_acc: 0.7719\n",
            "\n",
            "Epoch 00111: val_acc did not improve from 0.85958\n",
            "Epoch 112/120\n",
            "781/781 [==============================] - 395s 506ms/step - loss: 0.4895 - acc: 0.8494 - val_loss: 0.6007 - val_acc: 0.8393\n",
            "\n",
            "Epoch 00112: val_acc did not improve from 0.85958\n",
            "Epoch 113/120\n",
            "781/781 [==============================] - 401s 513ms/step - loss: 0.4836 - acc: 0.8519 - val_loss: 0.6385 - val_acc: 0.8298\n",
            "\n",
            "Epoch 00113: val_acc did not improve from 0.85958\n",
            "Epoch 114/120\n",
            "781/781 [==============================] - 395s 505ms/step - loss: 0.4822 - acc: 0.8517 - val_loss: 0.5446 - val_acc: 0.8502\n",
            "\n",
            "Epoch 00114: val_acc did not improve from 0.85958\n",
            "Epoch 115/120\n",
            "781/781 [==============================] - 395s 505ms/step - loss: 0.4737 - acc: 0.8539 - val_loss: 0.4963 - val_acc: 0.8613\n",
            "\n",
            "Epoch 00115: val_acc improved from 0.85958 to 0.86128, saving model to weights.115-0.86.hdf5\n",
            "Epoch 116/120\n",
            "781/781 [==============================] - 395s 506ms/step - loss: 0.4774 - acc: 0.8522 - val_loss: 0.5241 - val_acc: 0.8528\n",
            "\n",
            "Epoch 00116: val_acc did not improve from 0.86128\n",
            "Epoch 117/120\n",
            "781/781 [==============================] - 396s 507ms/step - loss: 0.4670 - acc: 0.8550 - val_loss: 0.5574 - val_acc: 0.8496\n",
            "\n",
            "Epoch 00117: val_acc did not improve from 0.86128\n",
            "Epoch 118/120\n",
            "781/781 [==============================] - 396s 507ms/step - loss: 0.4690 - acc: 0.8534 - val_loss: 0.5388 - val_acc: 0.8558\n",
            "\n",
            "Epoch 00118: val_acc did not improve from 0.86128\n",
            "Epoch 119/120\n",
            "781/781 [==============================] - 400s 512ms/step - loss: 0.4627 - acc: 0.8577 - val_loss: 0.4935 - val_acc: 0.8611\n",
            "\n",
            "Epoch 00119: val_acc did not improve from 0.86128\n",
            "Epoch 120/120\n",
            "781/781 [==============================] - 395s 506ms/step - loss: 0.4571 - acc: 0.8578 - val_loss: 0.4815 - val_acc: 0.8643\n",
            "\n",
            "Epoch 00120: val_acc improved from 0.86128 to 0.86428, saving model to weights.120-0.86.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5cd8999e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "eNn666ViBJil",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.010, decay=1e-4, momentum=0.9, nesterov=True) \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F9l-SjhCBSCV",
        "colab_type": "code",
        "outputId": "f012b5ed-1d59-49b8-f2ab-e6f4ee4a5028",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1822
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(data_generator,\n",
        "                   steps_per_epoch=50000//64,\n",
        "                    epochs= 145,\n",
        "                    verbose=1,\n",
        "                    initial_epoch = 120,\n",
        "                    callbacks=callbacks_list,\n",
        "                    validation_data=test_generator,validation_steps=10000//64)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 121/145\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py:799: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py:807: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 414s 530ms/step - loss: 0.4508 - acc: 0.8611 - val_loss: 0.4862 - val_acc: 0.8660\n",
            "\n",
            "Epoch 00121: val_acc improved from 0.86428 to 0.86599, saving model to weights.121-0.87.hdf5\n",
            "Epoch 122/145\n",
            "781/781 [==============================] - 394s 504ms/step - loss: 0.4476 - acc: 0.8631 - val_loss: 0.4770 - val_acc: 0.8676\n",
            "\n",
            "Epoch 00122: val_acc improved from 0.86599 to 0.86759, saving model to weights.122-0.87.hdf5\n",
            "Epoch 123/145\n",
            "781/781 [==============================] - 395s 506ms/step - loss: 0.4379 - acc: 0.8638 - val_loss: 0.5454 - val_acc: 0.8552\n",
            "\n",
            "Epoch 00123: val_acc did not improve from 0.86759\n",
            "Epoch 124/145\n",
            "781/781 [==============================] - 392s 502ms/step - loss: 0.4370 - acc: 0.8669 - val_loss: 0.5486 - val_acc: 0.8561\n",
            "\n",
            "Epoch 00124: val_acc did not improve from 0.86759\n",
            "Epoch 125/145\n",
            "781/781 [==============================] - 393s 503ms/step - loss: 0.4340 - acc: 0.8664 - val_loss: 0.4888 - val_acc: 0.8712\n",
            "\n",
            "Epoch 00125: val_acc improved from 0.86759 to 0.87119, saving model to weights.125-0.87.hdf5\n",
            "Epoch 126/145\n",
            "781/781 [==============================] - 393s 503ms/step - loss: 0.4288 - acc: 0.8670 - val_loss: 0.4517 - val_acc: 0.8785\n",
            "\n",
            "Epoch 00126: val_acc improved from 0.87119 to 0.87851, saving model to weights.126-0.88.hdf5\n",
            "Epoch 127/145\n",
            "781/781 [==============================] - 392s 502ms/step - loss: 0.4316 - acc: 0.8688 - val_loss: 0.4793 - val_acc: 0.8683\n",
            "\n",
            "Epoch 00127: val_acc did not improve from 0.87851\n",
            "Epoch 128/145\n",
            "781/781 [==============================] - 391s 501ms/step - loss: 0.4221 - acc: 0.8687 - val_loss: 0.4646 - val_acc: 0.8762\n",
            "\n",
            "Epoch 00128: val_acc did not improve from 0.87851\n",
            "Epoch 129/145\n",
            "781/781 [==============================] - 391s 500ms/step - loss: 0.4232 - acc: 0.8679 - val_loss: 0.4479 - val_acc: 0.8783\n",
            "\n",
            "Epoch 00129: val_acc did not improve from 0.87851\n",
            "Epoch 130/145\n",
            "781/781 [==============================] - 392s 502ms/step - loss: 0.4210 - acc: 0.8696 - val_loss: 0.4711 - val_acc: 0.8763\n",
            "\n",
            "Epoch 00130: val_acc did not improve from 0.87851\n",
            "Epoch 131/145\n",
            "781/781 [==============================] - 394s 504ms/step - loss: 0.4161 - acc: 0.8702 - val_loss: 0.4394 - val_acc: 0.8796\n",
            "\n",
            "Epoch 00131: val_acc improved from 0.87851 to 0.87961, saving model to weights.131-0.88.hdf5\n",
            "Epoch 132/145\n",
            "781/781 [==============================] - 393s 503ms/step - loss: 0.4152 - acc: 0.8714 - val_loss: 0.4228 - val_acc: 0.8822\n",
            "\n",
            "Epoch 00132: val_acc improved from 0.87961 to 0.88221, saving model to weights.132-0.88.hdf5\n",
            "Epoch 133/145\n",
            "781/781 [==============================] - 393s 503ms/step - loss: 0.4162 - acc: 0.8715 - val_loss: 0.4832 - val_acc: 0.8743\n",
            "\n",
            "Epoch 00133: val_acc did not improve from 0.88221\n",
            "Epoch 134/145\n",
            "781/781 [==============================] - 393s 503ms/step - loss: 0.4084 - acc: 0.8745 - val_loss: 0.4573 - val_acc: 0.8783\n",
            "\n",
            "Epoch 00134: val_acc did not improve from 0.88221\n",
            "Epoch 135/145\n",
            "781/781 [==============================] - 393s 503ms/step - loss: 0.4110 - acc: 0.8720 - val_loss: 0.4449 - val_acc: 0.8791\n",
            "\n",
            "Epoch 00135: val_acc did not improve from 0.88221\n",
            "Epoch 136/145\n",
            "781/781 [==============================] - 395s 505ms/step - loss: 0.4114 - acc: 0.8712 - val_loss: 0.4790 - val_acc: 0.8744\n",
            "\n",
            "Epoch 00136: val_acc did not improve from 0.88221\n",
            "Epoch 137/145\n",
            "781/781 [==============================] - 393s 503ms/step - loss: 0.4092 - acc: 0.8719 - val_loss: 0.4379 - val_acc: 0.8828\n",
            "\n",
            "Epoch 00137: val_acc improved from 0.88221 to 0.88281, saving model to weights.137-0.88.hdf5\n",
            "Epoch 138/145\n",
            "781/781 [==============================] - 394s 504ms/step - loss: 0.4067 - acc: 0.8728 - val_loss: 0.4470 - val_acc: 0.8826\n",
            "\n",
            "Epoch 00138: val_acc did not improve from 0.88281\n",
            "Epoch 139/145\n",
            "781/781 [==============================] - 391s 501ms/step - loss: 0.4025 - acc: 0.8747 - val_loss: 0.4451 - val_acc: 0.8816\n",
            "\n",
            "Epoch 00139: val_acc did not improve from 0.88281\n",
            "Epoch 140/145\n",
            "781/781 [==============================] - 392s 501ms/step - loss: 0.4065 - acc: 0.8730 - val_loss: 0.4650 - val_acc: 0.8764\n",
            "\n",
            "Epoch 00140: val_acc did not improve from 0.88281\n",
            "Epoch 141/145\n",
            "781/781 [==============================] - 393s 503ms/step - loss: 0.3997 - acc: 0.8757 - val_loss: 0.5083 - val_acc: 0.8685\n",
            "\n",
            "Epoch 00141: val_acc did not improve from 0.88281\n",
            "Epoch 142/145\n",
            "781/781 [==============================] - 392s 502ms/step - loss: 0.3967 - acc: 0.8769 - val_loss: 0.4230 - val_acc: 0.8889\n",
            "\n",
            "Epoch 00142: val_acc improved from 0.88281 to 0.88892, saving model to weights.142-0.89.hdf5\n",
            "Epoch 143/145\n",
            "781/781 [==============================] - 392s 502ms/step - loss: 0.3982 - acc: 0.8764 - val_loss: 0.4274 - val_acc: 0.8854\n",
            "\n",
            "Epoch 00143: val_acc did not improve from 0.88892\n",
            "Epoch 144/145\n",
            "781/781 [==============================] - 393s 503ms/step - loss: 0.3992 - acc: 0.8763 - val_loss: 0.4303 - val_acc: 0.8846\n",
            "\n",
            "Epoch 00144: val_acc did not improve from 0.88892\n",
            "Epoch 145/145\n",
            "781/781 [==============================] - 395s 506ms/step - loss: 0.3960 - acc: 0.8769 - val_loss: 0.4372 - val_acc: 0.8795\n",
            "\n",
            "Epoch 00145: val_acc did not improve from 0.88892\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5cd8999a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "U5wDH41cBjsw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.007, decay=1e-4, momentum=0.9, nesterov=True) \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uiNlZjk6BlZw",
        "colab_type": "code",
        "outputId": "7dfe21bd-d765-4eba-c4f6-8bdf5fc0999c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1482
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(data_generator,\n",
        "                   steps_per_epoch=50000//64,\n",
        "                    epochs= 165,\n",
        "                    verbose=1,\n",
        "                    initial_epoch = 145,\n",
        "                    callbacks=callbacks_list,\n",
        "                    validation_data=test_generator,validation_steps=10000//64)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 146/165\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py:799: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py:807: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 420s 538ms/step - loss: 0.4111 - acc: 0.8718 - val_loss: 0.4918 - val_acc: 0.8682\n",
            "\n",
            "Epoch 00146: val_acc did not improve from 0.88892\n",
            "Epoch 147/165\n",
            "781/781 [==============================] - 396s 506ms/step - loss: 0.4142 - acc: 0.8687 - val_loss: 0.5216 - val_acc: 0.8635\n",
            "\n",
            "Epoch 00147: val_acc did not improve from 0.88892\n",
            "Epoch 148/165\n",
            "781/781 [==============================] - 396s 507ms/step - loss: 0.4140 - acc: 0.8685 - val_loss: 0.4437 - val_acc: 0.8806\n",
            "\n",
            "Epoch 00148: val_acc did not improve from 0.88892\n",
            "Epoch 149/165\n",
            "781/781 [==============================] - 396s 507ms/step - loss: 0.4091 - acc: 0.8723 - val_loss: 0.4646 - val_acc: 0.8737\n",
            "\n",
            "Epoch 00149: val_acc did not improve from 0.88892\n",
            "Epoch 150/165\n",
            "781/781 [==============================] - 396s 507ms/step - loss: 0.4068 - acc: 0.8726 - val_loss: 0.4283 - val_acc: 0.8807\n",
            "\n",
            "Epoch 00150: val_acc did not improve from 0.88892\n",
            "Epoch 151/165\n",
            "781/781 [==============================] - 396s 507ms/step - loss: 0.4083 - acc: 0.8727 - val_loss: 0.4243 - val_acc: 0.8836\n",
            "\n",
            "Epoch 00151: val_acc did not improve from 0.88892\n",
            "Epoch 152/165\n",
            "781/781 [==============================] - 395s 506ms/step - loss: 0.4044 - acc: 0.8733 - val_loss: 0.4334 - val_acc: 0.8836\n",
            "\n",
            "Epoch 00152: val_acc did not improve from 0.88892\n",
            "Epoch 153/165\n",
            "781/781 [==============================] - 395s 506ms/step - loss: 0.3971 - acc: 0.8755 - val_loss: 0.4627 - val_acc: 0.8768\n",
            "\n",
            "Epoch 00153: val_acc did not improve from 0.88892\n",
            "Epoch 154/165\n",
            "781/781 [==============================] - 395s 506ms/step - loss: 0.4010 - acc: 0.8731 - val_loss: 0.4141 - val_acc: 0.8891\n",
            "\n",
            "Epoch 00154: val_acc improved from 0.88892 to 0.88912, saving model to weights.154-0.89.hdf5\n",
            "Epoch 155/165\n",
            "781/781 [==============================] - 399s 511ms/step - loss: 0.3930 - acc: 0.8754 - val_loss: 0.4922 - val_acc: 0.8725\n",
            "\n",
            "Epoch 00155: val_acc did not improve from 0.88912\n",
            "Epoch 156/165\n",
            "781/781 [==============================] - 398s 510ms/step - loss: 0.3954 - acc: 0.8751 - val_loss: 0.4266 - val_acc: 0.8846\n",
            "\n",
            "Epoch 00156: val_acc did not improve from 0.88912\n",
            "Epoch 157/165\n",
            "781/781 [==============================] - 397s 508ms/step - loss: 0.3951 - acc: 0.8774 - val_loss: 0.4285 - val_acc: 0.8855\n",
            "\n",
            "Epoch 00157: val_acc did not improve from 0.88912\n",
            "Epoch 158/165\n",
            "781/781 [==============================] - 398s 510ms/step - loss: 0.3944 - acc: 0.8750 - val_loss: 0.4630 - val_acc: 0.8768\n",
            "\n",
            "Epoch 00158: val_acc did not improve from 0.88912\n",
            "Epoch 159/165\n",
            "781/781 [==============================] - 397s 508ms/step - loss: 0.3901 - acc: 0.8768 - val_loss: 0.4017 - val_acc: 0.8896\n",
            "\n",
            "Epoch 00159: val_acc improved from 0.88912 to 0.88962, saving model to weights.159-0.89.hdf5\n",
            "Epoch 160/165\n",
            "781/781 [==============================] - 396s 508ms/step - loss: 0.3879 - acc: 0.8778 - val_loss: 0.4711 - val_acc: 0.8707\n",
            "\n",
            "Epoch 00160: val_acc did not improve from 0.88962\n",
            "Epoch 161/165\n",
            "781/781 [==============================] - 400s 513ms/step - loss: 0.3910 - acc: 0.8772 - val_loss: 0.4838 - val_acc: 0.8719\n",
            "\n",
            "Epoch 00161: val_acc did not improve from 0.88962\n",
            "Epoch 162/165\n",
            "781/781 [==============================] - 397s 508ms/step - loss: 0.3866 - acc: 0.8781 - val_loss: 0.4217 - val_acc: 0.8865\n",
            "\n",
            "Epoch 00162: val_acc did not improve from 0.88962\n",
            "Epoch 163/165\n",
            "781/781 [==============================] - 399s 511ms/step - loss: 0.3880 - acc: 0.8771 - val_loss: 0.4328 - val_acc: 0.8821\n",
            "\n",
            "Epoch 00163: val_acc did not improve from 0.88962\n",
            "Epoch 164/165\n",
            "781/781 [==============================] - 397s 508ms/step - loss: 0.3844 - acc: 0.8782 - val_loss: 0.4347 - val_acc: 0.8843\n",
            "\n",
            "Epoch 00164: val_acc did not improve from 0.88962\n",
            "Epoch 165/165\n",
            "781/781 [==============================] - 397s 508ms/step - loss: 0.3840 - acc: 0.8786 - val_loss: 0.4206 - val_acc: 0.8855\n",
            "\n",
            "Epoch 00165: val_acc did not improve from 0.88962\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5cd8999eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "aCUwHiJGgJ-8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_weights('weights.159-0.89.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GSV3K-IwjvkB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.004, decay=1e-4, momentum=0.9, nesterov=True) \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6gSTE-mZj46t",
        "colab_type": "code",
        "outputId": "dece6994-435d-4679-9a5d-ccb09f3691cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(data_generator,\n",
        "                   steps_per_epoch=50000//64,\n",
        "                    epochs= 172,\n",
        "                    verbose=1,\n",
        "                    initial_epoch = 165,\n",
        "                    callbacks=callbacks_list,\n",
        "                    validation_data=test_generator,validation_steps=10000//64)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 166/172\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py:799: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py:807: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 425s 544ms/step - loss: 0.3914 - acc: 0.8776 - val_loss: 0.4531 - val_acc: 0.8781\n",
            "\n",
            "Epoch 00166: val_acc improved from -inf to 0.87810, saving model to weights.166-0.88.hdf5\n",
            "Epoch 167/172\n",
            "781/781 [==============================] - 398s 510ms/step - loss: 0.3903 - acc: 0.8795 - val_loss: 0.4381 - val_acc: 0.8849\n",
            "\n",
            "Epoch 00167: val_acc improved from 0.87810 to 0.88492, saving model to weights.167-0.88.hdf5\n",
            "Epoch 168/172\n",
            "781/781 [==============================] - 399s 511ms/step - loss: 0.3887 - acc: 0.8775 - val_loss: 0.4494 - val_acc: 0.8820\n",
            "\n",
            "Epoch 00168: val_acc did not improve from 0.88492\n",
            "Epoch 169/172\n",
            "781/781 [==============================] - 398s 510ms/step - loss: 0.3860 - acc: 0.8783 - val_loss: 0.4457 - val_acc: 0.8847\n",
            "\n",
            "Epoch 00169: val_acc did not improve from 0.88492\n",
            "Epoch 170/172\n",
            "781/781 [==============================] - 398s 509ms/step - loss: 0.3838 - acc: 0.8777 - val_loss: 0.4308 - val_acc: 0.8844\n",
            "\n",
            "Epoch 00170: val_acc did not improve from 0.88492\n",
            "Epoch 171/172\n",
            "781/781 [==============================] - 398s 510ms/step - loss: 0.3794 - acc: 0.8810 - val_loss: 0.4210 - val_acc: 0.8853\n",
            "\n",
            "Epoch 00171: val_acc improved from 0.88492 to 0.88532, saving model to weights.171-0.89.hdf5\n",
            "Epoch 172/172\n",
            "781/781 [==============================] - 398s 510ms/step - loss: 0.3782 - acc: 0.8813 - val_loss: 0.4454 - val_acc: 0.8823\n",
            "\n",
            "Epoch 00172: val_acc did not improve from 0.88532\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb91e1df278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "40Vp3bFukHV0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.002, decay=1e-4, momentum=0.9, nesterov=True) \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QICKfA7yj9zB",
        "colab_type": "code",
        "outputId": "ee35607d-a415-47d6-ead2-aac4ce80e19a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(data_generator,\n",
        "                   steps_per_epoch=50000//64,\n",
        "                    epochs= 175,\n",
        "                    verbose=1,\n",
        "                    initial_epoch = 170,\n",
        "                    callbacks=callbacks_list,\n",
        "                    validation_data=test_generator,validation_steps=10000//64)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 171/175\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py:799: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py:807: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 424s 543ms/step - loss: 0.3776 - acc: 0.8816 - val_loss: 0.4098 - val_acc: 0.8909\n",
            "\n",
            "Epoch 00171: val_acc improved from 0.88532 to 0.89093, saving model to weights.171-0.89.hdf5\n",
            "Epoch 172/175\n",
            "781/781 [==============================] - 397s 508ms/step - loss: 0.3758 - acc: 0.8841 - val_loss: 0.4265 - val_acc: 0.8850\n",
            "\n",
            "Epoch 00172: val_acc did not improve from 0.89093\n",
            "Epoch 173/175\n",
            "781/781 [==============================] - 396s 507ms/step - loss: 0.3697 - acc: 0.8858 - val_loss: 0.4057 - val_acc: 0.8905\n",
            "\n",
            "Epoch 00173: val_acc did not improve from 0.89093\n",
            "Epoch 174/175\n",
            "781/781 [==============================] - 397s 508ms/step - loss: 0.3710 - acc: 0.8846 - val_loss: 0.4182 - val_acc: 0.8892\n",
            "\n",
            "Epoch 00174: val_acc did not improve from 0.89093\n",
            "Epoch 175/175\n",
            "781/781 [==============================] - 398s 509ms/step - loss: 0.3730 - acc: 0.8825 - val_loss: 0.4200 - val_acc: 0.8891\n",
            "\n",
            "Epoch 00175: val_acc did not improve from 0.89093\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb91826ad30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "metadata": {
        "id": "BUhcZ4qZ2jpY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_weights('weights.171-0.89.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kFgZJigbxVlg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.001, decay=1e-4, momentum=0.9, nesterov=True) \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b_wPiP23xUkJ",
        "colab_type": "code",
        "outputId": "195582d2-be42-4b6f-9b36-b4767aa503f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(data_generator,\n",
        "                   steps_per_epoch=50000//64,\n",
        "                    epochs= 175,\n",
        "                    verbose=1,\n",
        "                    initial_epoch = 170,\n",
        "                    callbacks=callbacks_list,\n",
        "                    validation_data=test_generator,validation_steps=10000//64)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 171/175\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py:799: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py:807: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 428s 548ms/step - loss: 0.3722 - acc: 0.8821 - val_loss: 0.4028 - val_acc: 0.8935\n",
            "\n",
            "Epoch 00171: val_acc improved from 0.89093 to 0.89353, saving model to weights.171-0.89.hdf5\n",
            "Epoch 172/175\n",
            "781/781 [==============================] - 397s 508ms/step - loss: 0.3663 - acc: 0.8852 - val_loss: 0.4135 - val_acc: 0.8929\n",
            "\n",
            "Epoch 00172: val_acc did not improve from 0.89353\n",
            "Epoch 173/175\n",
            "781/781 [==============================] - 397s 509ms/step - loss: 0.3658 - acc: 0.8853 - val_loss: 0.4156 - val_acc: 0.8909\n",
            "\n",
            "Epoch 00173: val_acc did not improve from 0.89353\n",
            "Epoch 174/175\n",
            "781/781 [==============================] - 397s 509ms/step - loss: 0.3614 - acc: 0.8875 - val_loss: 0.4130 - val_acc: 0.8919\n",
            "\n",
            "Epoch 00174: val_acc did not improve from 0.89353\n",
            "Epoch 175/175\n",
            "781/781 [==============================] - 397s 508ms/step - loss: 0.3667 - acc: 0.8866 - val_loss: 0.4062 - val_acc: 0.8930\n",
            "\n",
            "Epoch 00175: val_acc did not improve from 0.89353\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb9098dfbe0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "metadata": {
        "id": "vA7E0OYg-xTw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.003, decay=1e-4, momentum=0.9, nesterov=True) \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "28eVJOM1-4j_",
        "colab_type": "code",
        "outputId": "1b63e897-6494-4d0b-f595-00d6a67eb3f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(data_generator,\n",
        "                   steps_per_epoch=50000//64,\n",
        "                    epochs= 178,\n",
        "                    verbose=1,\n",
        "                    initial_epoch = 175,\n",
        "                    callbacks=callbacks_list,\n",
        "                    validation_data=test_generator,validation_steps=10000//64)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 176/178\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py:799: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py:807: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 428s 548ms/step - loss: 0.3764 - acc: 0.8825 - val_loss: 0.4536 - val_acc: 0.8817\n",
            "\n",
            "Epoch 00176: val_acc did not improve from 0.89353\n",
            "Epoch 177/178\n",
            "781/781 [==============================] - 395s 505ms/step - loss: 0.3769 - acc: 0.8808 - val_loss: 0.4711 - val_acc: 0.8786\n",
            "\n",
            "Epoch 00177: val_acc did not improve from 0.89353\n",
            "Epoch 178/178\n",
            "781/781 [==============================] - 394s 505ms/step - loss: 0.3776 - acc: 0.8819 - val_loss: 0.4332 - val_acc: 0.8843\n",
            "\n",
            "Epoch 00178: val_acc did not improve from 0.89353\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb903a50710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "metadata": {
        "id": "K_5TdJC3j9hf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.0009, decay=1e-4, momentum=0.9, nesterov=True) \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A06GKW6C-1Jc",
        "colab_type": "code",
        "outputId": "c34cf2a1-6308-4030-841a-185ee7ac197b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1142
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(data_generator,\n",
        "                   steps_per_epoch=50000//64,\n",
        "                    epochs= 193,\n",
        "                    verbose=1,\n",
        "                    initial_epoch = 178,\n",
        "                    callbacks=callbacks_list,\n",
        "                    validation_data=test_generator,validation_steps=10000//64)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 179/193\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py:799: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py:807: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 431s 552ms/step - loss: 0.3696 - acc: 0.8846 - val_loss: 0.4152 - val_acc: 0.8884\n",
            "\n",
            "Epoch 00179: val_acc did not improve from 0.89353\n",
            "Epoch 180/193\n",
            "781/781 [==============================] - 397s 509ms/step - loss: 0.3668 - acc: 0.8857 - val_loss: 0.4137 - val_acc: 0.8908\n",
            "\n",
            "Epoch 00180: val_acc did not improve from 0.89353\n",
            "Epoch 181/193\n",
            "781/781 [==============================] - 397s 508ms/step - loss: 0.3649 - acc: 0.8860 - val_loss: 0.4080 - val_acc: 0.8922\n",
            "\n",
            "Epoch 00181: val_acc did not improve from 0.89353\n",
            "Epoch 182/193\n",
            "781/781 [==============================] - 397s 508ms/step - loss: 0.3655 - acc: 0.8844 - val_loss: 0.4072 - val_acc: 0.8934\n",
            "\n",
            "Epoch 00182: val_acc did not improve from 0.89353\n",
            "Epoch 183/193\n",
            "781/781 [==============================] - 392s 502ms/step - loss: 0.3672 - acc: 0.8848 - val_loss: 0.4063 - val_acc: 0.8937\n",
            "\n",
            "Epoch 00183: val_acc improved from 0.89353 to 0.89373, saving model to weights.183-0.89.hdf5\n",
            "Epoch 184/193\n",
            "781/781 [==============================] - 392s 501ms/step - loss: 0.3632 - acc: 0.8867 - val_loss: 0.4008 - val_acc: 0.8947\n",
            "\n",
            "Epoch 00184: val_acc improved from 0.89373 to 0.89473, saving model to weights.184-0.89.hdf5\n",
            "Epoch 185/193\n",
            "781/781 [==============================] - 392s 502ms/step - loss: 0.3594 - acc: 0.8869 - val_loss: 0.4212 - val_acc: 0.8903\n",
            "\n",
            "Epoch 00185: val_acc did not improve from 0.89473\n",
            "Epoch 186/193\n",
            "781/781 [==============================] - 392s 502ms/step - loss: 0.3632 - acc: 0.8846 - val_loss: 0.4040 - val_acc: 0.8923\n",
            "\n",
            "Epoch 00186: val_acc did not improve from 0.89473\n",
            "Epoch 187/193\n",
            "781/781 [==============================] - 392s 502ms/step - loss: 0.3621 - acc: 0.8860 - val_loss: 0.3927 - val_acc: 0.8961\n",
            "\n",
            "Epoch 00187: val_acc improved from 0.89473 to 0.89613, saving model to weights.187-0.90.hdf5\n",
            "Epoch 188/193\n",
            "781/781 [==============================] - 392s 502ms/step - loss: 0.3582 - acc: 0.8862 - val_loss: 0.4045 - val_acc: 0.8931\n",
            "\n",
            "Epoch 00188: val_acc did not improve from 0.89613\n",
            "Epoch 189/193\n",
            "781/781 [==============================] - 392s 502ms/step - loss: 0.3568 - acc: 0.8875 - val_loss: 0.3968 - val_acc: 0.8961\n",
            "\n",
            "Epoch 00189: val_acc did not improve from 0.89613\n",
            "Epoch 190/193\n",
            "781/781 [==============================] - 391s 501ms/step - loss: 0.3575 - acc: 0.8868 - val_loss: 0.4034 - val_acc: 0.8942\n",
            "\n",
            "Epoch 00190: val_acc did not improve from 0.89613\n",
            "Epoch 191/193\n",
            "781/781 [==============================] - 392s 502ms/step - loss: 0.3587 - acc: 0.8870 - val_loss: 0.3965 - val_acc: 0.8966\n",
            "\n",
            "Epoch 00191: val_acc improved from 0.89613 to 0.89663, saving model to weights.191-0.90.hdf5\n",
            "Epoch 192/193\n",
            "781/781 [==============================] - 391s 501ms/step - loss: 0.3579 - acc: 0.8894 - val_loss: 0.4031 - val_acc: 0.8956\n",
            "\n",
            "Epoch 00192: val_acc did not improve from 0.89663\n",
            "Epoch 193/193\n",
            "781/781 [==============================] - 392s 502ms/step - loss: 0.3590 - acc: 0.8867 - val_loss: 0.3982 - val_acc: 0.8940\n",
            "\n",
            "Epoch 00193: val_acc did not improve from 0.89663\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb9098dfdd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "metadata": {
        "id": "0u4r7-lnUFUE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.0006, decay=1e-4, momentum=0.9, nesterov=True) \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rMEWMrJ2UFO4",
        "colab_type": "code",
        "outputId": "78ce7d1e-db04-47b8-dd79-83c201637e34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(data_generator,\n",
        "                   steps_per_epoch=50000//32,\n",
        "                    epochs= 199,\n",
        "                    verbose=1,\n",
        "                    initial_epoch = 193,\n",
        "                    callbacks=callbacks_list,\n",
        "                    validation_data=test_generator,validation_steps=10000//32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 194/199\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py:799: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py:807: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1562/1562 [==============================] - 826s 529ms/step - loss: 0.3629 - acc: 0.8861 - val_loss: 0.3962 - val_acc: 0.8945\n",
            "\n",
            "Epoch 00194: val_acc did not improve from 0.89663\n",
            "Epoch 195/199\n",
            "1562/1562 [==============================] - 785s 503ms/step - loss: 0.3605 - acc: 0.8860 - val_loss: 0.3919 - val_acc: 0.8952\n",
            "\n",
            "Epoch 00195: val_acc did not improve from 0.89663\n",
            "Epoch 196/199\n",
            "1562/1562 [==============================] - 787s 504ms/step - loss: 0.3569 - acc: 0.8875 - val_loss: 0.3995 - val_acc: 0.8945\n",
            "\n",
            "Epoch 00196: val_acc did not improve from 0.89663\n",
            "Epoch 197/199\n",
            "1562/1562 [==============================] - 791s 507ms/step - loss: 0.3568 - acc: 0.8876 - val_loss: 0.3964 - val_acc: 0.8952\n",
            "\n",
            "Epoch 00197: val_acc did not improve from 0.89663\n",
            "Epoch 198/199\n",
            "1562/1562 [==============================] - 783s 501ms/step - loss: 0.3559 - acc: 0.8885 - val_loss: 0.3905 - val_acc: 0.8977\n",
            "\n",
            "Epoch 00198: val_acc improved from 0.89663 to 0.89769, saving model to weights.198-0.90.hdf5\n",
            "Epoch 199/199\n",
            "1562/1562 [==============================] - 781s 500ms/step - loss: 0.3563 - acc: 0.8873 - val_loss: 0.3974 - val_acc: 0.8960\n",
            "\n",
            "Epoch 00199: val_acc did not improve from 0.89769\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb8e838f390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "metadata": {
        "id": "rrPVBE_888wo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_weights('weights.198-0.90.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1pEb05Dk81EX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.0003, decay=1e-4, momentum=0.9, nesterov=True) \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5q7DfRas89sr",
        "colab_type": "code",
        "outputId": "2bdcfc13-c0a9-42be-e383-606761fcc063",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(data_generator,\n",
        "                   steps_per_epoch=50000//64,\n",
        "                    epochs= 209,\n",
        "                    verbose=1,\n",
        "                    initial_epoch = 199,\n",
        "                    callbacks=callbacks_list,\n",
        "                    validation_data=test_generator,validation_steps=10000//64)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 200/209\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py:799: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py:807: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 441s 564ms/step - loss: 0.3528 - acc: 0.8888 - val_loss: 0.3903 - val_acc: 0.8981\n",
            "\n",
            "Epoch 00200: val_acc improved from 0.89769 to 0.89814, saving model to weights.200-0.90.hdf5\n",
            "Epoch 201/209\n",
            "649/781 [=======================>......] - ETA: 1:02 - loss: 0.3542 - acc: 0.8889"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7vFUOwxRrrZl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.00035, decay=1e-4, momentum=0.9, nesterov=True) \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WG0GFk86VFCD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_weights('weights.200-0.90.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CGUvnliVTkgl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "92adb534-7054-445f-ccee-6a75ee3ed8e9"
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(data_generator,\n",
        "                   steps_per_epoch=50000//64,\n",
        "                    epochs= 209,\n",
        "                    verbose=1,\n",
        "                    initial_epoch = 201,\n",
        "                    callbacks=callbacks_list,\n",
        "                    validation_data=test_generator,validation_steps=10000//64)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 202/209\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py:799: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py:807: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 403s 516ms/step - loss: 0.3543 - acc: 0.8892 - val_loss: 0.3938 - val_acc: 0.8965\n",
            "\n",
            "Epoch 00202: val_acc improved from -inf to 0.89653, saving model to weights.202-0.90.hdf5\n",
            "Epoch 203/209\n",
            "781/781 [==============================] - 376s 482ms/step - loss: 0.3533 - acc: 0.8874 - val_loss: 0.3912 - val_acc: 0.8969\n",
            "\n",
            "Epoch 00203: val_acc improved from 0.89653 to 0.89694, saving model to weights.203-0.90.hdf5\n",
            "Epoch 204/209\n",
            "781/781 [==============================] - 376s 482ms/step - loss: 0.3553 - acc: 0.8885 - val_loss: 0.3921 - val_acc: 0.8958\n",
            "\n",
            "Epoch 00204: val_acc did not improve from 0.89694\n",
            "Epoch 205/209\n",
            "781/781 [==============================] - 377s 482ms/step - loss: 0.3532 - acc: 0.8890 - val_loss: 0.3872 - val_acc: 0.8975\n",
            "\n",
            "Epoch 00205: val_acc improved from 0.89694 to 0.89754, saving model to weights.205-0.90.hdf5\n",
            "Epoch 206/209\n",
            "781/781 [==============================] - 376s 482ms/step - loss: 0.3557 - acc: 0.8866 - val_loss: 0.3896 - val_acc: 0.8985\n",
            "\n",
            "Epoch 00206: val_acc improved from 0.89754 to 0.89854, saving model to weights.206-0.90.hdf5\n",
            "Epoch 207/209\n",
            "781/781 [==============================] - 377s 482ms/step - loss: 0.3531 - acc: 0.8895 - val_loss: 0.3821 - val_acc: 0.8993\n",
            "\n",
            "Epoch 00207: val_acc improved from 0.89854 to 0.89934, saving model to weights.207-0.90.hdf5\n",
            "Epoch 208/209\n",
            "781/781 [==============================] - 376s 482ms/step - loss: 0.3522 - acc: 0.8901 - val_loss: 0.3885 - val_acc: 0.8970\n",
            "\n",
            "Epoch 00208: val_acc did not improve from 0.89934\n",
            "Epoch 209/209\n",
            "781/781 [==============================] - 377s 483ms/step - loss: 0.3556 - acc: 0.8871 - val_loss: 0.3989 - val_acc: 0.8955\n",
            "\n",
            "Epoch 00209: val_acc did not improve from 0.89934\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc8967652e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "X_HHvOpbW9CP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.00015, decay=1e-4, momentum=0.9, nesterov=True) \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wVD-fTmFrrMm",
        "colab_type": "code",
        "outputId": "f230db98-25ac-4ea6-92df-f2f7b9ad101b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 808
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train,\n",
        "                    batch_size=64,\n",
        "                    epochs=219,\n",
        "                    verbose=1,\n",
        "                    initial_epoch=209,\n",
        "                    callbacks=callbacks_list,\n",
        "                    validation_data=(x_test, y_test))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 210/219\n",
            "50000/50000 [==============================] - 394s 8ms/step - loss: 0.2803 - acc: 0.9136 - val_loss: 0.3525 - val_acc: 0.9013\n",
            "\n",
            "Epoch 00210: val_acc improved from 0.89934 to 0.90130, saving model to weights.210-0.90.hdf5\n",
            "Epoch 211/219\n",
            "50000/50000 [==============================] - 365s 7ms/step - loss: 0.2745 - acc: 0.9165 - val_loss: 0.3494 - val_acc: 0.9024\n",
            "\n",
            "Epoch 00211: val_acc improved from 0.90130 to 0.90240, saving model to weights.211-0.90.hdf5\n",
            "Epoch 212/219\n",
            "50000/50000 [==============================] - 365s 7ms/step - loss: 0.2664 - acc: 0.9192 - val_loss: 0.3513 - val_acc: 0.9029\n",
            "\n",
            "Epoch 00212: val_acc improved from 0.90240 to 0.90290, saving model to weights.212-0.90.hdf5\n",
            "Epoch 213/219\n",
            "50000/50000 [==============================] - 366s 7ms/step - loss: 0.2638 - acc: 0.9205 - val_loss: 0.3517 - val_acc: 0.9028\n",
            "\n",
            "Epoch 00213: val_acc did not improve from 0.90290\n",
            "Epoch 214/219\n",
            "50000/50000 [==============================] - 365s 7ms/step - loss: 0.2652 - acc: 0.9186 - val_loss: 0.3498 - val_acc: 0.9037\n",
            "\n",
            "Epoch 00214: val_acc improved from 0.90290 to 0.90370, saving model to weights.214-0.90.hdf5\n",
            "Epoch 215/219\n",
            "50000/50000 [==============================] - 365s 7ms/step - loss: 0.2600 - acc: 0.9210 - val_loss: 0.3486 - val_acc: 0.9030\n",
            "\n",
            "Epoch 00215: val_acc did not improve from 0.90370\n",
            "Epoch 216/219\n",
            "50000/50000 [==============================] - 366s 7ms/step - loss: 0.2595 - acc: 0.9212 - val_loss: 0.3484 - val_acc: 0.9047\n",
            "\n",
            "Epoch 00216: val_acc improved from 0.90370 to 0.90470, saving model to weights.216-0.90.hdf5\n",
            "Epoch 217/219\n",
            "50000/50000 [==============================] - 365s 7ms/step - loss: 0.2616 - acc: 0.9204 - val_loss: 0.3481 - val_acc: 0.9041\n",
            "\n",
            "Epoch 00217: val_acc did not improve from 0.90470\n",
            "Epoch 218/219\n",
            "50000/50000 [==============================] - 365s 7ms/step - loss: 0.2592 - acc: 0.9213 - val_loss: 0.3510 - val_acc: 0.9036\n",
            "\n",
            "Epoch 00218: val_acc did not improve from 0.90470\n",
            "Epoch 219/219\n",
            "50000/50000 [==============================] - 365s 7ms/step - loss: 0.2605 - acc: 0.9213 - val_loss: 0.3497 - val_acc: 0.9045\n",
            "\n",
            "Epoch 00219: val_acc did not improve from 0.90470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc86dc28cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "DtGzIA7R6TfU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(horizontal_flip=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aYOd0Isdtrlv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_generator = datagen.flow(x_train, y_train, batch_size=64)\n",
        "test_generator = ImageDataGenerator().flow(x_test, y_test, batch_size=64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cOeKrzkats9J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.0001, decay=1e-4, momentum=0.9, nesterov=True) \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BdTJwbym1u7V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*To speedup training the batch size is increased to 128 and this also boosted accuracy.*"
      ]
    },
    {
      "metadata": {
        "id": "4-EInhlKtwJr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "outputId": "440c784e-9516-4388-9e23-aba2ad3ca95d"
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(data_generator,\n",
        "                   steps_per_epoch=50000//128,\n",
        "                    epochs= 229,\n",
        "                    verbose=1,\n",
        "                    initial_epoch = 219,\n",
        "                    callbacks=callbacks_list,\n",
        "                    validation_data=test_generator,validation_steps=10000//128)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 220/229\n",
            "390/390 [==============================] - 186s 476ms/step - loss: 0.2590 - acc: 0.9226 - val_loss: 0.3449 - val_acc: 0.9052\n",
            "\n",
            "Epoch 00220: val_acc improved from 0.90470 to 0.90525, saving model to weights.220-0.91.hdf5\n",
            "Epoch 221/229\n",
            "390/390 [==============================] - 184s 471ms/step - loss: 0.2635 - acc: 0.9200 - val_loss: 0.3452 - val_acc: 0.9050\n",
            "\n",
            "Epoch 00221: val_acc did not improve from 0.90525\n",
            "Epoch 222/229\n",
            "390/390 [==============================] - 183s 470ms/step - loss: 0.2543 - acc: 0.9241 - val_loss: 0.3447 - val_acc: 0.9044\n",
            "\n",
            "Epoch 00222: val_acc did not improve from 0.90525\n",
            "Epoch 223/229\n",
            "390/390 [==============================] - 183s 469ms/step - loss: 0.2616 - acc: 0.9207 - val_loss: 0.3438 - val_acc: 0.9050\n",
            "\n",
            "Epoch 00223: val_acc did not improve from 0.90525\n",
            "Epoch 224/229\n",
            "390/390 [==============================] - 183s 469ms/step - loss: 0.2663 - acc: 0.9199 - val_loss: 0.3444 - val_acc: 0.9050\n",
            "\n",
            "Epoch 00224: val_acc did not improve from 0.90525\n",
            "Epoch 225/229\n",
            "390/390 [==============================] - 183s 469ms/step - loss: 0.2592 - acc: 0.9215 - val_loss: 0.3428 - val_acc: 0.9056\n",
            "\n",
            "Epoch 00225: val_acc improved from 0.90525 to 0.90565, saving model to weights.225-0.91.hdf5\n",
            "Epoch 226/229\n",
            "390/390 [==============================] - 183s 470ms/step - loss: 0.2647 - acc: 0.9201 - val_loss: 0.3423 - val_acc: 0.9062\n",
            "\n",
            "Epoch 00226: val_acc improved from 0.90565 to 0.90625, saving model to weights.226-0.91.hdf5\n",
            "Epoch 227/229\n",
            "390/390 [==============================] - 183s 470ms/step - loss: 0.2561 - acc: 0.9232 - val_loss: 0.3441 - val_acc: 0.9054\n",
            "\n",
            "Epoch 00227: val_acc did not improve from 0.90625\n",
            "Epoch 228/229\n",
            "390/390 [==============================] - 184s 471ms/step - loss: 0.2587 - acc: 0.9208 - val_loss: 0.3424 - val_acc: 0.9069\n",
            "\n",
            "Epoch 00228: val_acc improved from 0.90625 to 0.90685, saving model to weights.228-0.91.hdf5\n",
            "Epoch 229/229\n",
            "390/390 [==============================] - 183s 470ms/step - loss: 0.2606 - acc: 0.9204 - val_loss: 0.3456 - val_acc: 0.9069\n",
            "\n",
            "Epoch 00229: val_acc did not improve from 0.90685\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc85fca4be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "BF0B-VTXw80J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.00012, decay=1e-4, momentum=0.9, nesterov=True) \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yj3O8b68w8Vl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "8845e78a-4380-41d5-bf47-ef7b3ae4cc05"
      },
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train,\n",
        "                    batch_size=128,\n",
        "                    epochs=234,\n",
        "                    verbose=1,\n",
        "                    initial_epoch=229,\n",
        "                    callbacks=callbacks_list,\n",
        "                    validation_data=(x_test, y_test))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 230/234\n",
            "50000/50000 [==============================] - 335s 7ms/step - loss: 0.2485 - acc: 0.9245 - val_loss: 0.3496 - val_acc: 0.9049\n",
            "\n",
            "Epoch 00230: val_acc did not improve from 0.90685\n",
            "Epoch 231/234\n",
            "50000/50000 [==============================] - 328s 7ms/step - loss: 0.2463 - acc: 0.9256 - val_loss: 0.3508 - val_acc: 0.9044\n",
            "\n",
            "Epoch 00231: val_acc did not improve from 0.90685\n",
            "Epoch 232/234\n",
            "50000/50000 [==============================] - 328s 7ms/step - loss: 0.2479 - acc: 0.9253 - val_loss: 0.3511 - val_acc: 0.9044\n",
            "\n",
            "Epoch 00232: val_acc did not improve from 0.90685\n",
            "Epoch 233/234\n",
            "50000/50000 [==============================] - 328s 7ms/step - loss: 0.2443 - acc: 0.9275 - val_loss: 0.3495 - val_acc: 0.9056\n",
            "\n",
            "Epoch 00233: val_acc did not improve from 0.90685\n",
            "Epoch 234/234\n",
            "50000/50000 [==============================] - 329s 7ms/step - loss: 0.2445 - acc: 0.9255 - val_loss: 0.3502 - val_acc: 0.9052\n",
            "\n",
            "Epoch 00234: val_acc did not improve from 0.90685\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc858eea358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "TOPAVvtg8uCt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.000095, decay=1e-4, momentum=0.9, nesterov=True) \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wAAcN95o87Jg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(data_generator,\n",
        "                   steps_per_epoch=50000//128,\n",
        "                    epochs= 239,\n",
        "                    verbose=1,\n",
        "                    initial_epoch = 234,\n",
        "                    callbacks=callbacks_list,\n",
        "                    validation_data=test_generator,validation_steps=10000//128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ne_6-hoE9E_1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.00005, decay=1e-4, momentum=0.9, nesterov=True) \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CmDRzaM89NjU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train,\n",
        "                    batch_size=128,\n",
        "                    epochs=250,\n",
        "                    verbose=1,\n",
        "                    initial_epoch=239,\n",
        "                    callbacks=callbacks_list,\n",
        "                    validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZcWydmIVhZGr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UE3lF6EH1r_L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save the trained weights in to .h5 format\n",
        "model.save_weights(\"DNST_model.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ai-yZ2ED5AK1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('DNST_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Og56VCRh5j8V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}