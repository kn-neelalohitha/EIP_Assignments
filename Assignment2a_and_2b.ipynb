{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2a and 2b",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJmSc9ED3CeW",
        "colab_type": "text"
      },
      "source": [
        "#Assignment 2A\n",
        "Name : Neelalohitha K.N. \n",
        "Batch : 3\n",
        "\n",
        "# Backpropagation Step by Step\n",
        "---\n",
        "<center>![](https://hmkcode.github.io/images/ai/backpropagation.png)</center>\n",
        "\n",
        "If you are building your own neural network, you will definitely need to understand how to train it. Backpropagation is a commonly used technique for training neural network. There are many resources explaining the technique, but this post will explain backpropagation with concrete example in a very detailed colorful steps.\n",
        "<br>\n",
        "\n",
        "## Overview\n",
        "---\n",
        "In this post, we will build a neural network with three layers:\n",
        "<ul>\n",
        "  <li><strong>Input</strong> layer with two inputs neurons</li>\n",
        "  <li>One <strong>hidden</strong> layer with two neurons</li>\n",
        "  <li><strong>Output</strong> layer with a single neuron</li>\n",
        "</ul>\n",
        "\n",
        "<center>![](https://hmkcode.github.io/images/ai/nn1.png)</center>\n",
        "\n",
        "## Weights, weights, weights\n",
        "___\n",
        "\n",
        "Neural network training is about finding weights that minimize prediction error. We usually start our training with a set of randomly generated weights.Then, backpropagation is used to update the weights in an attempt to correctly map arbitrary inputs to outputs.\n",
        "\n",
        "Our initial weights will be as following:<br>\n",
        "```w1 = 0.07, w2 = 0.26, w3 = 0.13, w4 = 0.17, w5 = 0.18 and w6 = 0.24```\n",
        "<br>\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} \n",
        "  2  & 3 \\\\ \n",
        "  \\end{bmatrix}\n",
        "  .\n",
        "  \\begin{bmatrix}\n",
        "  0.07 & 0.13 \\\\\n",
        "  0.26 & 0.17\n",
        "  \\end{bmatrix}\n",
        "  = \n",
        "  \\begin{bmatrix}\n",
        "  0.92 & 0.77 \\\\\n",
        "  \\end{bmatrix}\n",
        "  .\n",
        "  \\begin{bmatrix}\n",
        "  0.18 \\\\\n",
        "  0.24\n",
        "  \\end{bmatrix}\n",
        "  =\n",
        "  0.3504\n",
        " $$ \n",
        " \n",
        " ## Dataset\n",
        " ---\n",
        "Our dataset has one sample with two inputs and one output.\n",
        "<center>![](https://hmkcode.github.io/images/ai/bp_dataset.png)</center>\n",
        "Our single sample is as following inputs=[2, 3] and output=[1].\n",
        "<center>![](http://hmkcode.github.io/images/ai/bp_sample.png)</center>\n",
        " \n",
        " ## Forward Pass\n",
        " ---\n",
        " We will use given weights and inputs to predict the output. Inputs are multiplied by weights; the results are then passed forward to next layer.\n",
        " <br>\n",
        " \n",
        " ## Calculating Error\n",
        " ---\n",
        " Now, it’s time to find out how our network performed by calculating the difference between the actual output and predicted one. It’s clear that our network output, or **prediction**, is not even close to **actual output**. We can calculate the difference or the error as following.\n",
        " \n",
        "## Reducing Error\n",
        "---\n",
        "Our main goal of the training is to reduce the **error** or the difference between **prediction** and **actual output**. Since **actual output** is constant, “not changing”, the only way to reduce the error is to change prediction value. The question now is, how to change **prediction** value?\n",
        "\n",
        "By decomposing **prediction** into its basic elements we can find that **weights** are the variable elements affecting **prediction** value. In other words, in order to change **prediction** value, we need to change **weights** values.\n",
        "\n",
        " ![](http://hmkcode.github.io/images/ai/bp_prediction_elements.png) <br>\n",
        "> The question now is **how to change\\update the weights value so that the error is reduced?**\n",
        "The answer is **Backpropagation!**\n",
        "\n",
        "\n",
        "## Backpropagation\n",
        "---\n",
        "**Backpropagation**, short for “backward propagation of errors”, is a mechanism used to update the **weights** using [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent). It calculates the gradient of the error function with respect to the neural network’s weights. The calculation proceeds backwards through the network.\n",
        "\n",
        "> **Gradient descent** is an iterative optimization algorithm for finding the minimum of a function; in our case we want to minimize th error function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient of the function at the current point.\n",
        "<center>![bp_update_formula](https://hmkcode.github.io/images/ai/bp_update_formula.png)</center>\n",
        "\n",
        "For example, to update `w6`, we take the current `w6` and subtract the partial derivative of **error** function with respect to `w6`. Optionally, we multiply the derivative of the **error** function by a selected number to make sure that the new updated **weight** is minimizing the **error** function; this number is called ***learning rate***.\n",
        "<center> ![](https://hmkcode.github.io/images/ai/bp_w6_update.png) </center>\n",
        "\n",
        "The derivation of the error function is evaluated by applying the chain rule as following\n",
        "<center>![](https://hmkcode.github.io/images/ai/bp_error_function_partial_derivative_w6.png)</center>\n",
        "So to update w6 we can apply the following formula\n",
        "<center>\n",
        "![](http://hmkcode.github.io/images/ai/bp_w6_update_closed_form.png)\n",
        "</center>\n",
        "Similarly, we can derive the update formula for `w5` and any other weights existing between the output and the hidden layer.\n",
        "\n",
        "<center>![](http://hmkcode.github.io/images/ai/bp_w5_update_closed_form.png)\n",
        "</center>\n",
        "However, when moving backward to update ```w1, w2, w3 and w4``` existing between input and hidden layer, the partial derivative for the error function with respect to `w1`, for example, will be as following.\n",
        "<center>![](http://hmkcode.github.io/images/ai/bp_error_function_partial_derivative_w1.png)</center>\n",
        "We can find the update formula for the remaining weights ```w2, w3 and w4``` in the same way.\n",
        "In summary, the update formulas for all weights will be as following:\n",
        "<center>![](http://hmkcode.github.io/images/ai/bp_update_all_weights.png)</center>\n",
        "We can rewrite the update formulas in matrices as following\n",
        "<center>![](http://hmkcode.github.io/images/ai/bp_update_all_weights_matrix.png)</center>\n",
        "\n",
        "##Backward Pass\n",
        "---\n",
        "Using derived formulas we can find the new **weights**.\n",
        "\n",
        "> **Learning rate**: is a hyperparameter which means that we need to manually guess its value.\n",
        "\n",
        "$$\\bbox[2px,border:2px  dashed grey] {\\Delta = 0.3504 - 1 = -0.6496} \\leftarrow \\boldsymbol{Delta} =Prediction - Actual $$ \n",
        "\n",
        "$$\\bbox[2px,border:2px  dashed grey]{\n",
        "a = 0.05\n",
        "} \\leftarrow \\boldsymbol{Learning\\space rate},we\\space smartly \\space guess\\space this \\space number$$\n",
        "\n",
        "Now, using the new **weights** we will repeat the forward passed\n",
        "$$\\begin{bmatrix}\n",
        "w5 \\\\\n",
        "w6\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "0.18 \\\\\n",
        "0.24\n",
        "\\end{bmatrix} - 0.05(-0.6496)\\begin{bmatrix}\n",
        "0.92 \\\\\n",
        "0.77\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "0.18 \\\\\n",
        "0.24\n",
        "\\end{bmatrix}\n",
        "-\n",
        "\\begin{bmatrix}\n",
        "-0.0299 \\\\\n",
        "-0.0250 \\\\\n",
        "\\end{bmatrix}=\n",
        "\\begin{bmatrix}\n",
        "0.2099\\\\\n",
        "0.2650\\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "W1 & W3\\\\\n",
        "W2 & W4\\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "  \\begin{bmatrix}\n",
        "  0.07 & 0.13 \\\\\n",
        "  0.26 & 0.17\n",
        "  \\end{bmatrix}\n",
        "  - 0.05(-0.6496)\\begin{bmatrix}\n",
        "2 \\\\\n",
        "3 \\\\\n",
        "\\end{bmatrix}\n",
        ".\n",
        "\\begin{bmatrix}\n",
        "0.18 &\n",
        "0.24\n",
        "\\end{bmatrix}\n",
        "=\n",
        "  \\begin{bmatrix}\n",
        "  0.07 & 0.13 \\\\\n",
        "  0.26 & 0.17\n",
        "  \\end{bmatrix}\n",
        "  -\n",
        "    \\begin{bmatrix}\n",
        "  -0.0117 & -0.0156 \\\\\n",
        "  -0.0175 & -0.0234\n",
        "  \\end{bmatrix}\n",
        "  =\n",
        "    \\begin{bmatrix}\n",
        "  0.0817 & 0.1456 \\\\\n",
        "  0.2775 & 0.1934\\\\\n",
        "  \\end{bmatrix}\n",
        "$$\n",
        "<br>\n",
        "Now, using the new weights we will repeat the forward passed.\n",
        "<br>\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} \n",
        "  2  & 3 \\\\ \n",
        "  \\end{bmatrix}\n",
        "  .\n",
        " \\begin{bmatrix}\n",
        "  0.0817 & 0.1456 \\\\\n",
        "  0.2775 & 0.1934\\\\\n",
        "  \\end{bmatrix}\n",
        "  = \n",
        "  \\begin{bmatrix}\n",
        "  0.9959 & 0.8714 \\\\\n",
        "  \\end{bmatrix}\n",
        "  .\n",
        "\\begin{bmatrix}\n",
        "0.2099\\\\\n",
        "0.2650\\\\\n",
        "\\end{bmatrix}\n",
        "  =\n",
        "  0.44\n",
        " $$ \n",
        " <br>\n",
        " We can notice that the **prediction** `0.44` is a little bit closer to **actual output** than the previously predicted one `0.3504`. We can repeat the same process of backward and forward pass until **error** is close or equal to zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAaA_Kte0kjf",
        "colab_type": "code",
        "outputId": "54518f9d-4dbb-4150-9c23-7c43df4d906e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "# Assignment 2B\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "I = np.array([2,3]) #input\n",
        "print('input',I)\n",
        "out = [1] # output\n",
        "print('out = ',out)\n",
        "\n",
        "# Initial Weights\n",
        "wa = np.array([[.07, 0.13], [0.26, 0.17]]) # weights w1,w2,w3,w4\n",
        "wb = np.array([0.18, 0.24]) # weights w5,w6\n",
        "print('wa = \\n',wa)\n",
        "print('wb = \\n',wb)\n",
        "\n",
        "#hidden layer\n",
        "h = np.dot(I,wa)\n",
        "print('hidden layer',h)\n",
        "\n",
        "#Forward Pass-1\n",
        "prediction_out = np.dot(h,wb)\n",
        "print('prediction = ',prediction_out)\n",
        "\n",
        "\n",
        "#Difference between actual output and prediction after 1st forward pass\n",
        "\n",
        "delta_error = np.subtract(prediction_out,out)\n",
        "print('Delta_error',delta_error)\n",
        "a = 0.05 # Learning rate\n",
        "\n",
        "#Weight(w5, w6) updation after 1st Backpropagation\n",
        "wb_new = np.subtract(wb,a *delta_error *h)\n",
        "print('wb after 1st backpropagation =\\n',wb_new)\n",
        "\n",
        "#Weight(w1, w2, w3, w4) updation after 1st backpropagation\n",
        "wa_new =wa - a * delta_error * np.dot(I.reshape(2,1),wb.reshape(1,2)) # The array is reshaped to perform [2 X 1] X [1 X 2] matrix multiplication\n",
        "print('wa after 1st backpropagation =')\n",
        "print(wa_new)\n",
        "\n",
        "#Hidden layer ,forward pass-2 \n",
        "h = np.dot(I,wa_new)\n",
        "print('hidden layer after 2nd Forward pass',h)\n",
        "\n",
        "#Prediction after Forward Pass2 (and 1st backpropagation)\n",
        "prediction_out = np.dot(h,wb_new)\n",
        "print('prediction after 2nd Forward pass = ',prediction_out)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input [2 3]\n",
            "out =  [1]\n",
            "wa = \n",
            " [[0.07 0.13]\n",
            " [0.26 0.17]]\n",
            "wb = \n",
            " [0.18 0.24]\n",
            "hidden layer [0.92 0.77]\n",
            "prediction =  0.3504\n",
            "Delta_error [-0.6496]\n",
            "wb after 1st backpropagation =\n",
            " [0.2098816 0.2650096]\n",
            "wa after 1st backpropagation =\n",
            "[[0.0816928 0.1455904]\n",
            " [0.2775392 0.1933856]]\n",
            "hidden layer after 2nd Forward pass [0.9960032 0.8713376]\n",
            "prediction after 2nd Forward pass =  0.43995557406207997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ipaIjjb4BaJ",
        "colab_type": "text"
      },
      "source": [
        "The same code is split into different cells to show the outputs at different stages "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDsAp70p3Apa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-p8hyQkIo5Y",
        "colab_type": "code",
        "outputId": "fb146888-d869-4eb7-8c72-27ab3046b5b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "I = np.array([2,3]) #input\n",
        "print('input = ',I)\n",
        "out = [1] # output\n",
        "print('out = ',out)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input =  [2 3]\n",
            "out =  [1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RdGYp4YJKA7",
        "colab_type": "code",
        "outputId": "54a22bab-4fbb-4e8e-8484-1154e7e6b754",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Initial Weights\n",
        "wa = np.array([[.07, 0.13], [0.26, 0.17]]) # weights w1,w2,w3,w4\n",
        "wb = np.array([0.18, 0.24]) # weights w5,w6\n",
        "print('wa =\\n',wa)\n",
        "print('wb =\\n',wb)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wa =\n",
            " [[0.07 0.13]\n",
            " [0.26 0.17]]\n",
            "wb =\n",
            " [0.18 0.24]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQsBfE_ZJ05B",
        "colab_type": "code",
        "outputId": "d43750a8-4c33-44f9-c3ff-30863ec7c64f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#hidden layer\n",
        "h = np.dot(I,wa)\n",
        "print('hidden layer',h)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hidden layer [0.92 0.77]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPWYqTp2LIKo",
        "colab_type": "code",
        "outputId": "16f912e5-67c8-4bc8-b396-efafd380ad01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Forward Pass-1\n",
        "prediction_out = np.dot(h,wb)\n",
        "print('prediction = ',prediction_out)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prediction =  0.3504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyEuTjOVLJII",
        "colab_type": "code",
        "outputId": "b1a97143-595c-4d29-d356-b8a673db1ab8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Difference between actual output and prediction after 1st forward pass\n",
        "delta_error = np.subtract(prediction_out,out)\n",
        "print('Delta_error',delta_error)\n",
        "a = 0.05 # Learning Rate"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Delta_error [-0.6496]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbgVu5LGMOFW",
        "colab_type": "code",
        "outputId": "b1e3d2cb-4b69-4bf7-9d5e-c46c554a477b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Weight(w5, w6) updation after 1st Backpropagation\n",
        "wb_new = np.subtract(wb,a *delta_error *h)\n",
        "print('wb after 1st backpropagation =\\n',wb_new)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wb after 1st backpropagation =\n",
            " [0.2098816 0.2650096]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64gzP1HOPXls",
        "colab_type": "code",
        "outputId": "2c4756ad-742d-4854-af2f-42d47780baf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Weight(w1, w2, w3, w4) updation after 1st backpropagation\n",
        "wa_new =wa - a * delta_error * np.dot(I.reshape(2,1),wb.reshape(1,2)) # The array is reshaped to perform [2 X 1] X [1 X 2] matrix multiplication\n",
        "print('wa after 1st backpropagation =')\n",
        "print(wa_new)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wa after 1st backpropagation =\n",
            "[[0.0816928 0.1455904]\n",
            " [0.2775392 0.1933856]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orRDukUCQC3w",
        "colab_type": "code",
        "outputId": "8ee0c23d-7b13-4327-95fe-04e5dec77de7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Hidden layer ,forward pass-2 \n",
        "h = np.dot(I,wa_new)\n",
        "print('hidden layer after 2nd Forward pass',h)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hidden layer after 2nd Forward pass [0.9960032 0.8713376]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCte-cr4pyvQ",
        "colab_type": "code",
        "outputId": "960c9d5a-3c92-4032-c811-ca64f99c0a3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Prediction after Forward Pass2 (and 1st backpropagation)\n",
        "prediction_out = np.dot(h,wb_new)\n",
        "print('prediction after 2nd Forward pass = ',prediction_out)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prediction after 2nd Forward pass =  0.43995557406207997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bRKQURWqXMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}